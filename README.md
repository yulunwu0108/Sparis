<h2 align="center">
    Sparis: Neural Implicit Surface Reconstruction of Indoor Scenes from Sparse Views
</h2>
<h4 align="center">AAAI 2025 Oral</h4>
<div align="center">
    <a href='https://yulunwu0108.github.io/' target='_blank'>Yulun Wu</a>*&emsp;
    <a href='https://github.com/alvin528' target='_blank'>Han Huang</a>*&emsp;
    <a href='https://wen-yuan-zhang.github.io/' target='_blank'>Wenyuan Zhang</a>&emsp;
    Chao Deng&emsp;
    Ge Gao&dagger;&emsp;
    Ming Gu&emsp;
    <a href='https://yushen-liu.github.io/' target='_blank'>Yu-Shen Liu</a><br>
    Tsinghua University<br>
    <small>*Equal contribution.&emsp;&dagger;Corresponding author.</small>
</div>
<p align="center">
  <a href="https://arxiv.org/abs/2501.01196" target='_blank'><img src="http://img.shields.io/badge/cs.CV-arXiv%3A2501.01196-b31b1b"></a>
  <a href="https://yulunwu0108.github.io/Sparis/" target='_blank'><img src="http://img.shields.io/badge/Project_Page-ðŸ›‹-lightblue"></a>
</p>

## TODO

- [ ] Code release

## Overview

<div align=center>  <img src="./media/overview.png" width=100%> </div>

In this paper, we propose a new method, named Sparis, for indoor surface reconstruction from sparse views. Specifically, we investigate the impact of monocular priors on sparse scene reconstruction, introducing a novel prior based on inter-image matching information. Our prior offers more accurate depth information while ensuring cross-view matching consistency.

## Citation

If you find our work useful in your research, please consider citing:

```bibtex
@inproceedings{wu2025sparis,
    title={Sparis: Neural Implicit Surface Reconstruction of Indoor Scenes from Sparse Views},
    author={Yulun Wu and Han Huang and Wenyuan Zhang and Chao Deng and Ge Gao and Ming Gu and Yu-Shen Liu},
    booktitle={AAAI Conference on Artificial Intelligence},
    year={2025}
}
```
